---
title: "APIs and Web Scraping Lab"
date: "`r Sys.Date()`"
author: "YOUR NAME HERE"
output: html_document
urlcolor: "blue"
params: 
  solutions: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = params$solutions, 
                      eval = params$solutions,
                      fig.height = 3, 
                      fig.width  = 6,
                      fig.align  = "center")
ggplot2::theme_set(ggplot2::theme_bw())
```

# Learning Objectives

- Obtain data from an API.
- Scrape data from the web.
- Intense data cleaning.

# Semantic Scholar

[Semantic Scholar](https://www.semanticscholar.org/) is like Google Scholar, but they claim to return more useful connections between journal articles and authors. Recently, I've found myself using both, whereas before I spent most of my time searching for articles on Google Scholar.

Semantic Scholar has a really nice API to practice on: <https://api.semanticscholar.org/>

I actually used this API in real-life when I was preparing my reappointment files to check out and plot citation counts. Let's try and reproduce what I did.

Just like Google Scholar, each author as their own page. Here is mine: <https://www.semanticscholar.org/author/David-Gerard/145899953>

1. Consider the following vector of [DOIs](https://en.wikipedia.org/wiki/Digital_object_identifier) of my publications:

    ```{r, echo = TRUE, eval = TRUE}
    gerard_dois <- c("10.1186/s12859-020-3450-9",
                     "10.1093/bioinformatics/btz852",
                     "10.1080/07391102.2019.1679666",
                     "10.1093/biostatistics/kxy029",
                     "10.1534/genetics.118.301468",
                     "10.1214/17-EJS1330",
                     "10.5705/ss.202018.0345",
                     "10.1016/j.laa.2016.04.033",
                     "10.1016/j.jmva.2015.01.020",
                     "10.1007/s11084-013-9331-8",
                     "10.1186/1471-2148-11-291")
    ```

    Use the API to download the paper information for each of my publications listed above. Save the content of this information as a list. So, for example, each element of this list is itself a list with the following elements:

2. Tidy these data. Extract the DOI's, title, and authors from each paper. Also, for each year, calculate the number of citations each paper received. So your final data frame should look like this:

3. Plot the cumulative number of citations over time, aggregating over all papers. Your plot should look like this:

4. For each paper, make a plot of cumulative summations (use a for-loop). Your plots should look like this:

# Film Remakes

Consider the list of film remakes from Wikipedia: 
<https://en.wikipedia.org/wiki/List_of_film_remakes_(A-M)> and
<https://en.wikipedia.org/wiki/List_of_film_remakes_(N-Z)>

1. Download the html file and save it as a variable. You can also load the
   "remakes_1.html" and "remakes_2.html" files in the data folder.

2. Extract the "table.wikitable" elements from both files.

3. Now obtain a single list of all of the table elements.

4. Create a single data frame with two columns --- `Remakes` and `Original version`.

5. Create a data frame that contains the year of the remake, the year of the
   original, and the name of the original. Note that there are many films with
   multiple remakes.
   
    Your final data frame should look like this:

6. There are two films remade in the same year as the original version. What
   were they?

7. Find the 5 movies that were remade the most number of times. You should get:

8. Plot a step function for these movies by year. Your final plot should look
   like this:
